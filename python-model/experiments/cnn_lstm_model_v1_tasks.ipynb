{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "# To unzip the edf_dataset\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# EDFlib and Data Preprocesing module\n",
    "from mne.preprocessing import ICA, create_eog_epochs\n",
    "import mne\n",
    "from pyedflib import highlevel\n",
    "import pyedflib as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def download_file(url, save_path):\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"File already exists at '{save_path}'. Skipping download.\")\n",
    "        return  # Exit the function if the file exists\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open the file in binary write mode and save the content\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully and saved to '{save_path}'\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "# Specify the URL and the path where you want to save the file\n",
    "url = 'https://figshare.com/ndownloader/articles/4244171/versions/2'\n",
    "# Change this to your desired path\n",
    "save_path = './edf_dataset.zip'\n",
    "\n",
    "# Call the function to download the file\n",
    "download_file(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(zip_file_path, extract_to_folder):\n",
    "\n",
    "    # Check if the directory exist\n",
    "    \n",
    "    if os.path.exists(extract_to_folder):\n",
    "        print(f\"Directory '{extract_to_folder} already exists\")\n",
    "        return # Exit the function if the directory\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extract_to_folder, exist_ok=True)\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extract all the contents into the specified folder\n",
    "        zip_ref.extractall(extract_to_folder)\n",
    "\n",
    "\n",
    "# Specify the path to the zip file and the extraction folder\n",
    "zip_file_path = './edf_dataset.zip'\n",
    "# Change this if needed\n",
    "extract_to_folder = './edf_dataset_2'\n",
    "\n",
    "# Call the function to unzip\n",
    "unzip_file(zip_file_path, extract_to_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "edf_directory = \"./edf_dataset_2\"\n",
    "\n",
    "for filename in os.listdir(edf_directory):\n",
    "    new_filename = filename\n",
    "    \n",
    "    # Handle spaces in filename\n",
    "    if ' ' in new_filename:\n",
    "        new_filename = re.sub(r'\\s+', '_', new_filename)\n",
    "    \n",
    "    # Handle subject numbers (S1 -> S01)\n",
    "    new_filename = re.sub(r'S(\\d)(?!\\d)', r'S0\\1', new_filename)\n",
    "    \n",
    "    # Handle filenames containing '6931959'\n",
    "    if '6931959' in new_filename:\n",
    "        new_filename = new_filename.replace('6931959_', '')\n",
    "    \n",
    "    # Handle filenames containing '6921143'\n",
    "    if '6921143' in new_filename:\n",
    "        new_filename = new_filename.replace('6921143_', '')\n",
    "    \n",
    "    # Zero-pad other single-digit numbers in filenames\n",
    "    new_filename = re.sub(r'(?<!\\d)(\\d)(?!\\d)', r'0\\1', new_filename)\n",
    "    \n",
    "    # Only rename if the filename has changed\n",
    "    if new_filename != filename:\n",
    "        old_file = os.path.join(edf_directory, filename)\n",
    "        new_file = os.path.join(edf_directory, new_filename)\n",
    "        \n",
    "        os.rename(old_file, new_file)\n",
    "        print(f'Renamed: \"{filename}\" to \"{new_filename}\"')\n",
    "\n",
    "print(\"Renaming complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_task_type(filename):\n",
    "    if '_EC.' in filename:\n",
    "        return 0  # Eyes Closed\n",
    "    elif '_EO.' in filename:\n",
    "        return 1  # Eyes Open\n",
    "    elif '_TASK.' in filename:\n",
    "        return 2  # Task\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type in filename: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename channels and drop specified channels based on conditions\n",
    "def process_channels(raw_data):\n",
    "    \"\"\"\n",
    "    Process and standardize EEG channels to keep only the 17 most common channels.\n",
    "    \"\"\"\n",
    "    print(f\"Initial channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Initialize a list to hold channels to drop\n",
    "    channels_to_drop = []\n",
    "\n",
    "    # Create mapping for channel renaming\n",
    "    rename_map = {}\n",
    "    for name in raw_data.ch_names:\n",
    "        if any(x in name for x in ['23A-23R', '24A-24R', 'A2-A1']):\n",
    "            channels_to_drop.append(name)\n",
    "        else:\n",
    "            new_name = name.replace('EEG ', '').replace('-LE', '')\n",
    "            rename_map[name] = new_name\n",
    "\n",
    "    # Drop unwanted channels\n",
    "    if channels_to_drop:\n",
    "        print(f\"Dropping channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Rename remaining channels\n",
    "    raw_data.rename_channels(rename_map)\n",
    "\n",
    "    print(f\"Final channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Define the 17 most common channels\n",
    "    expected_channels = [\n",
    "        'Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'Fp2',\n",
    "        'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz'\n",
    "    ]\n",
    "\n",
    "    # Keep only the expected channels\n",
    "    channels_to_keep = set(expected_channels)\n",
    "    channels_to_drop = [\n",
    "        ch for ch in raw_data.ch_names if ch not in channels_to_keep]\n",
    "\n",
    "    if channels_to_drop:\n",
    "        print(\n",
    "            f\"Dropping channels to keep only the expected 17 channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Verify we have the expected number of channels (should be 17)\n",
    "    if len(raw_data.ch_names) != len(expected_channels):\n",
    "        print(\n",
    "            f\"Warning: Expected {len(expected_channels)} channels, got {len(raw_data.ch_names)}\")\n",
    "        print(f\"Missing: {set(expected_channels) - set(raw_data.ch_names)}\")\n",
    "\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edf_files = os.listdir(edf_directory)\n",
    "ec_file_path = [i for i in all_edf_files if i.endswith('EC.edf')]\n",
    "eo_file_path = [i for i in all_edf_files if i.endswith('EO.edf')]\n",
    "task_file_path = [i for i in all_edf_files if i.endswith('TASK.edf')]\n",
    "\n",
    "print(len(all_edf_files), len(ec_file_path), len(eo_file_path), len(task_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(data, l_freq, h_freq):\n",
    "    # Adjust the filter parameters as needed\n",
    "    data.filter(l_freq=l_freq, h_freq=h_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ICA(raw, n_components ):\n",
    "    print(f\"Preprocessing ICA: {raw.filenames}\")\n",
    "\n",
    "    ica = ICA(n_components=n_components,random_state=97,\n",
    "              max_iter=800) \n",
    "    ica.fit(raw)\n",
    "    return ica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epochs(processed_data, duration=5.0, overlap=1.0):\n",
    "    \"\"\"\n",
    "    Create epochs from continuous EEG data and format for CNN input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_data : mne.io.Raw\n",
    "        The raw EEG data\n",
    "    duration : float\n",
    "        Duration of each epoch in seconds\n",
    "    overlap : float\n",
    "        Overlap between epochs in seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_array : numpy.ndarray\n",
    "        The epoched data formatted for CNN (samples, channels, timepoints, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create epochs\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        processed_data,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    # Drop bad epochs\n",
    "    epochs.drop_bad()\n",
    "\n",
    "    # Get data and reshape for CNN\n",
    "    # Shape will be (n_epochs, n_channels, n_timepoints)\n",
    "    data = epochs.get_data()\n",
    "\n",
    "    # Add channel dimension for CNN: (n_epochs, n_channels, n_timepoints, 1)\n",
    "    data = data[..., np.newaxis]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the EDF files\n",
    "edf_directory = \"./edf_dataset_2\"  # Adjust this path to your dataset location\n",
    "\n",
    "# Initialize lists\n",
    "processed_raw_data = []\n",
    "class_counts = {'Healthy': 0, 'MDD': 0}\n",
    "\n",
    "# Read all EDF files\n",
    "for filename in os.listdir(edf_directory):\n",
    "    if filename.endswith('.edf'):\n",
    "        file_path = os.path.join(edf_directory, filename)\n",
    "        try:\n",
    "            # Read the raw data\n",
    "            raw_data = read_data(file_path)\n",
    "\n",
    "            if raw_data is not None:\n",
    "                processed_raw_data.append(raw_data)\n",
    "                print(f\"Successfully loaded: {filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to load: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nTotal files loaded: {len(processed_raw_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Preprocessing Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eeg(raw_data, l_freq=0.5, h_freq=50.0, n_components=5, time_steps=5, samples_per_step=256):\n",
    "    \"\"\"\n",
    "    Complete EEG preprocessing pipeline: bandpass -> ICA -> reshape for CNN-LSTM\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nProcessing file: {raw_data.filenames}\")\n",
    "        \n",
    "        processed_raw = process_channels(raw_data=raw_data)\n",
    "\n",
    "        # 1. Bandpass filtering\n",
    "        print(\"1. Applying bandpass filter...\")\n",
    "        bandpass_filter(processed_raw, l_freq, h_freq)\n",
    "\n",
    "        # 2. ICA\n",
    "        print(\"2. Applying ICA...\")\n",
    "        ica = preprocess_ICA(processed_raw, n_components)\n",
    "        ica.apply(processed_raw)\n",
    "\n",
    "        # 3. Reshape data for CNN-LSTM\n",
    "        print(\"3. Reshaping data for CNN-LSTM...\")\n",
    "        data = processed_raw.get_data()\n",
    "        n_channels = data.shape[0]\n",
    "        \n",
    "        # Calculate total samples needed for each complete sequence\n",
    "        total_samples_per_sequence = time_steps * samples_per_step\n",
    "        \n",
    "        # Determine how many complete sequences we can make\n",
    "        n_sequences = data.shape[1] // total_samples_per_sequence\n",
    "        \n",
    "        # Reshape the data\n",
    "        reshaped_data = []\n",
    "        for i in range(n_sequences):\n",
    "            start = i * total_samples_per_sequence\n",
    "            end = start + total_samples_per_sequence\n",
    "            sequence = data[:, start:end]\n",
    "            \n",
    "            # Reshape into (time_steps, channels, samples_per_step)\n",
    "            sequence = sequence.reshape(time_steps, n_channels, samples_per_step)\n",
    "            \n",
    "            # Transpose to (time_steps, channels, samples_per_step)\n",
    "            sequence = np.transpose(sequence, (0, 2, 1))\n",
    "            \n",
    "            reshaped_data.append(sequence)\n",
    "        \n",
    "        # Convert to numpy array and add channel dimension\n",
    "        final_data = np.array(reshaped_data)[..., np.newaxis]\n",
    "        \n",
    "        print(f\"Reshaping completed. Final data shape: {final_data.shape}\")\n",
    "        return final_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Preprocessing error: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now process each raw data file\n",
    "X_data = []\n",
    "y_labels = []\n",
    "task_labels = [] # For EC/EO/TASK\n",
    "class_counts = {'Healthy': 0, 'MDD': 0}\n",
    "\n",
    "print(\"\\nStarting preprocessing pipeline...\")\n",
    "for raw_data in processed_raw_data:\n",
    "    filename = os.path.basename(raw_data.filenames[0])\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing: {filename}\")\n",
    "    print(f\"Initial data info:\")\n",
    "    print(f\"Channels: {raw_data.ch_names}\")\n",
    "    print(f\"Sample rate: {raw_data.info['sfreq']} Hz\")\n",
    "    print(f\"Duration: {raw_data.n_times / raw_data.info['sfreq']:.2f} seconds\")\n",
    "    \n",
    "    # Extract both condition and task labels\n",
    "    condition_label = 1 if filename.startswith('MDD') else 0\n",
    "    task_label = extract_task_type(filename)\n",
    "\n",
    "    # Apply complete preprocessing pipeline\n",
    "    try:\n",
    "        # First, process the channels\n",
    "        # Make a copy to prevent modifying original\n",
    "        raw_data = process_channels(raw_data.copy()) \n",
    "        print(f\"Channels after processing: {raw_data.ch_names}\")\n",
    "\n",
    "        # Apply complete preprocessing pipeline\n",
    "        processed_data = preprocess_eeg(raw_data, l_freq=0.5, h_freq=50.0,\n",
    "                                        n_components=5, time_steps=5, samples_per_step=256)\n",
    "        \n",
    "\n",
    "        if processed_data is not None:\n",
    "            X_data.append(processed_data)\n",
    "            \n",
    "            # Create label (1 for MDD, 0 for Healthy)\n",
    "            label = 1 if filename.startswith('MDD') else 0\n",
    "            y_labels.extend([label] * processed_data.shape[0])\n",
    "\n",
    "            # Update counts with number of epochs\n",
    "            if label == 1:\n",
    "                class_counts['MDD'] += processed_data.shape[0]\n",
    "            else:\n",
    "                class_counts['Healthy'] += processed_data.shape[0]\n",
    "            \n",
    "            # Create task label based on filename\n",
    "            if '_EC.' in filename:\n",
    "                task = 0\n",
    "            elif '_EO.' in filename:\n",
    "                task = 1\n",
    "            elif '_TASK.' in filename:\n",
    "                task = 2\n",
    "                \n",
    "            # Make sure to extend task labels for each epoch\n",
    "            task_labels.extend([task] * processed_data.shape[0])\n",
    "\n",
    "           \n",
    "\n",
    "            print(f\"Successfully processed {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to process {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total files processed: {len(processed_raw_data)}\")\n",
    "print(f\"Total epochs: {len(y_labels)}\")\n",
    "print(f\"Successfully processed files: {len(X_data)}\")\n",
    "print(f\"Failed files: {len(processed_raw_data) - len(X_data)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"MDD epochs: {class_counts['MDD']}\")\n",
    "print(f\"Healthy epochs: {class_counts['Healthy']}\")\n",
    "# print(f\"\\nChannels used: {raw_data.ch_names}\")\n",
    "\n",
    "# # Optional: Print class balance percentage\n",
    "# total_epochs = class_counts['MDD'] + class_counts['Healthy']\n",
    "# print(\"\\nClass balance:\")\n",
    "# print(f\"MDD: {(class_counts['MDD']/total_epochs)*100:.2f}%\")\n",
    "# print(f\"Healthy: {(class_counts['Healthy']/total_epochs)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_data) > 0:\n",
    "    try:\n",
    "        # Print shapes before concatenation\n",
    "        print(\"\\nArray shapes before concatenation:\")\n",
    "        for i, arr in enumerate(X_data):\n",
    "            print(f\"Array {i}: shape {arr.shape}\")\n",
    "\n",
    "        # Extract task information from filenames\n",
    "        task_labels = []\n",
    "        for raw_data in processed_raw_data:\n",
    "            filename = os.path.basename(raw_data.filenames[0])\n",
    "            if '_EC.' in filename:\n",
    "                task = 0  # Eyes Closed\n",
    "            elif '_EO.' in filename:\n",
    "                task = 1  # Eyes Open\n",
    "            elif '_TASK.' in filename:\n",
    "                task = 2  # Task\n",
    "\n",
    "            # Extend task labels for each epoch in the processed data\n",
    "            if processed_data is not None:\n",
    "                task_labels.extend([task] * processed_data.shape[0])\n",
    "\n",
    "        # Concatenate data\n",
    "        X = np.concatenate(X_data, axis=0)\n",
    "        y = np.array(y_labels)\n",
    "        tasks = np.array(task_labels)\n",
    "\n",
    "        # Print final information\n",
    "        print(\"\\nFinal Dataset Information:\")\n",
    "        print(f\"Total samples: {len(X)}\")\n",
    "        print(f\"Healthy samples: {class_counts['Healthy']}\")\n",
    "        print(f\"MDD samples: {class_counts['MDD']}\")\n",
    "        print(f\"Input shape: {X.shape}\")\n",
    "        print(f\"Labels shape: {y.shape}\")\n",
    "        print(f\"Task labels shape: {tasks.shape}\")\n",
    "\n",
    "        # Print task distribution\n",
    "        print(\"\\nTask distribution:\")\n",
    "        print(f\"Eyes Closed (EC): {np.sum(tasks == 0)}\")\n",
    "        print(f\"Eyes Open (EO): {np.sum(tasks == 1)}\")\n",
    "        print(f\"Task: {np.sum(tasks == 2)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during final processing: {str(e)}\")\n",
    "        print(\"Checking individual arrays for inconsistencies...\")\n",
    "        # Find arrays with different shapes\n",
    "        base_shape = X_data[0].shape[1:]\n",
    "        for i, arr in enumerate(X_data):\n",
    "            if arr.shape[1:] != base_shape:\n",
    "                print(\n",
    "                    f\"Mismatch at index {i}: expected {base_shape}, got {arr.shape[1:]}\")\n",
    "else:\n",
    "    print(\"\\nNo data was successfully processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task_labels[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, LSTM, Dropout, Flatten, Reshape, TimeDistributed\n",
    "\n",
    "\n",
    "def create_2dcnn_lstm_model_with_task(input_shape=(5, 256, 17, 1)):\n",
    "    # Main input for EEG data\n",
    "    eeg_input = Input(shape=input_shape, name='eeg_input')\n",
    "\n",
    "    # Task input (for EC, EO, TASK)\n",
    "    task_input = Input(shape=(1,), name='task_input')\n",
    "\n",
    "    # EEG processing branch\n",
    "    x = TimeDistributed(\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'))(eeg_input)\n",
    "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "    x = TimeDistributed(\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "    x = TimeDistributed(\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "\n",
    "    # Flatten CNN output while preserving time steps\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    # LSTM layers\n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    x = LSTM(64)(x)\n",
    "\n",
    "    # Combine with task information\n",
    "    task_embedding = Dense(32, activation='relu')(task_input)\n",
    "\n",
    "    # Concatenate EEG features with task embedding\n",
    "    combined = tf.keras.layers.Concatenate()([x, task_embedding])\n",
    "\n",
    "    # Dense layers\n",
    "    x = Dense(128, activation='relu')(combined)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create model with multiple inputs\n",
    "    model = Model(inputs=[eeg_input, task_input], outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def prepare_data_with_task(X_data, y_labels, task_labels):\n",
    "    # Combine all data\n",
    "    X = np.concatenate(X_data, axis=0)\n",
    "    y = np.array(y_labels)\n",
    "    tasks = np.array(task_labels)\n",
    "\n",
    "    # Find the minimum length among all arrays\n",
    "    min_length = min(X.shape[0], len(y), len(tasks))\n",
    "\n",
    "    # Trim all arrays to the same length\n",
    "    X = X[:min_length]\n",
    "    y = y[:min_length]\n",
    "    tasks = tasks[:min_length]\n",
    "\n",
    "    print(\"\\nAfter trimming:\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"tasks shape: {tasks.shape}\")\n",
    "\n",
    "    # Normalize the EEG data\n",
    "    X = (X - X.mean()) / X.std()\n",
    "\n",
    "    # Train-test split while maintaining stratification\n",
    "    X_train, X_test, y_train, y_test, task_train, task_test = train_test_split(\n",
    "        X, y, tasks, test_size=0.2, random_state=42,\n",
    "        stratify=np.column_stack((y, tasks))\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, task_train, task_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_task(model, X_train, task_train, y_train, X_test, task_test, y_test):\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=5, restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train model with multiple inputs\n",
    "    history = model.fit(\n",
    "        {'eeg_input': X_train, 'task_input': task_train},\n",
    "        y_train,\n",
    "        validation_data=(\n",
    "            {'eeg_input': X_test, 'task_input': task_test}, y_test),\n",
    "        epochs=25,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape checks:\")\n",
    "print(f\"X_data length: {len(X_data)}\")\n",
    "print(f\"y_labels length: {len(y_labels)}\")\n",
    "print(f\"task_labels length: {len(task_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_model_with_task(model, X_train, task_train, y_train, X_test, task_test, y_test):\n",
    "    # Training predictions\n",
    "    y_train_pred = model.predict({\n",
    "        'eeg_input': X_train,\n",
    "        'task_input': task_train\n",
    "    })\n",
    "    y_train_pred_classes = (y_train_pred > 0.5).astype(int)\n",
    "\n",
    "    # Test predictions\n",
    "    y_test_pred = model.predict({\n",
    "        'eeg_input': X_test,\n",
    "        'task_input': task_test\n",
    "    })\n",
    "    y_test_pred_classes = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'accuracy': accuracy_score(y_train, y_train_pred_classes),\n",
    "            'precision': precision_score(y_train, y_train_pred_classes),\n",
    "            'recall': recall_score(y_train, y_train_pred_classes),\n",
    "            'f1': f1_score(y_train, y_train_pred_classes),\n",
    "            'auc': roc_auc_score(y_train, y_train_pred)\n",
    "        },\n",
    "        'test': {\n",
    "            'accuracy': accuracy_score(y_test, y_test_pred_classes),\n",
    "            'precision': precision_score(y_test, y_test_pred_classes),\n",
    "            'recall': recall_score(y_test, y_test_pred_classes),\n",
    "            'f1': f1_score(y_test, y_test_pred_classes),\n",
    "            'auc': roc_auc_score(y_test, y_test_pred)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    for dataset in ['train', 'test']:\n",
    "        print(f\"\\n{dataset.capitalize()} Results:\")\n",
    "        for metric, value in metrics[dataset].items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "    # Plot confusion matrices\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Training confusion matrix\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred_classes)\n",
    "    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Greens', ax=ax1)\n",
    "    ax1.set_title('Training Confusion Matrix')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "    # Testing confusion matrix\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred_classes)\n",
    "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
    "    ax2.set_title('Testing Confusion Matrix')\n",
    "    ax2.set_ylabel('True Label')\n",
    "    ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your preprocessing code\n",
    "X_train, X_test, y_train, y_test, task_train, task_test = prepare_data_with_task(\n",
    "    X_data, y_labels, task_labels\n",
    ")\n",
    "\n",
    "# Create and train model\n",
    "#input_shape = X_train.shape[1:]  # (channels, height, width)\n",
    "input_shape = (5, 256, 17, 1)\n",
    "model = create_2dcnn_lstm_model_with_task(input_shape)\n",
    "history = train_model_with_task(\n",
    "    model, X_train, task_train, y_train, X_test, task_test, y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training your task-aware model\n",
    "metrics = evaluate_model_with_task(\n",
    "    model,              # Your trained task-aware model\n",
    "    X_train,           # Training EEG data\n",
    "    task_train,        # Training task labels (EC=0, EO=1, TASK=2)\n",
    "    y_train,           # Training class labels (MDD/Healthy)\n",
    "    X_test,            # Test EEG data\n",
    "    task_test,         # Test task labels\n",
    "    y_test             # Test class labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_training_history_with_task(history, y_test, X_test, task_test, model):\n",
    "    # Get predictions for ROC curve\n",
    "    y_pred = model.predict({\n",
    "        'eeg_input': X_test,\n",
    "        'task_input': task_test\n",
    "    })\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Plot 1: ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    axes[0].plot(fpr, tpr, 'b-', label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    axes[0].plot([0, 1], [0, 1], 'r--')\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curve')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot 2: Accuracy\n",
    "    axes[1].plot(history.history['accuracy'], 'b-', label='Training')\n",
    "    axes[1].plot(history.history['val_accuracy'], 'r-', label='Validation')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Model Accuracy')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # Plot 3: Loss\n",
    "    axes[2].plot(history.history['loss'], 'b-', label='Training')\n",
    "    axes[2].plot(history.history['val_loss'], 'r-', label='Validation')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].set_title('Model Loss')\n",
    "    axes[2].legend(loc='upper right')\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training your task-aware model\n",
    "plot_training_history_with_task(\n",
    "    history,              # Training history from model.fit\n",
    "    y_test,              # Test labels\n",
    "    X_test,              # Test EEG data\n",
    "    task_test,           # Test task labels (EC=0, EO=1, TASK=2)\n",
    "    model                # Your trained task-aware model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predicting with the task-aware model\n",
    "predictions = model.predict({\n",
    "    'eeg_input': X_test[:100],  # First 100 EEG samples\n",
    "    'task_input': task_test[:100]  # First 100 task labels\n",
    "})\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(\"First few predictions:\", predictions[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
