{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for model at: ./saved_model/1d_cnn/1st_model/\n",
      "Directory exists: True\n",
      "Model loaded successfully\n",
      "Extracting EDF parameters from /Users/hansandreanto/Development/capstone-project/edf_dataset_2/MDD_S6_EC.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 77311  =      0.000 ...   301.996 secs...\n",
      "1. Applying bandpass filter...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 60 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 60.00 Hz\n",
      "- Upper transition bandwidth: 15.00 Hz (-6 dB cutoff frequency: 67.50 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n",
      "Setting up band-stop filter from 48 - 52 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 48.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 48.25 Hz)\n",
      "- Upper passband edge: 51.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 51.75 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n",
      "Bandpass filtering completed\n",
      "2. Removing bad channels...\n",
      "Initial channels: ['EEG Fp1-LE', 'EEG F3-LE', 'EEG C3-LE', 'EEG P3-LE', 'EEG O1-LE', 'EEG F7-LE', 'EEG T3-LE', 'EEG T5-LE', 'EEG Fz-LE', 'EEG Fp2-LE', 'EEG F4-LE', 'EEG C4-LE', 'EEG P4-LE', 'EEG O2-LE', 'EEG F8-LE', 'EEG T4-LE', 'EEG T6-LE', 'EEG Cz-LE', 'EEG Pz-LE', 'EEG A2-A1']\n",
      "Dropping channels: ['EEG A2-A1']\n",
      "Final channels: ['Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'T5', 'Fz', 'Fp2', 'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz']\n",
      "Dropping channels to keep only the expected 17 channels: ['T5', 'Fz']\n",
      "Bad channels removed\n",
      "3. Creating epochs...\n",
      "Not setting metadata\n",
      "75 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 75 events and 1280 original time points ...\n",
      "0 bad epochs dropped\n",
      "Epoching completed. Final data shape: (75, 17, 1280, 1)\n",
      "4. Applying ICA...\n",
      "Preprocessing ICA for 75 epochs...\n",
      "Fitting ICA to data using 2 channels (please be patient, this may take a while)\n",
      "Selecting by number: 2 components\n",
      "Fitting ICA took 0.1s.\n",
      "Applying ICA to Epochs instance\n",
      "    Transforming to ICA space (2 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 2 PCA components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1008: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('Setting up band-pass filter from %0.2g - %0.2g Hz'\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1008: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('Setting up band-pass filter from %0.2g - %0.2g Hz'\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1724: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  l_freq = cast(l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1729: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  h_freq = cast(h_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1034: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' from %0.2g - %0.2g Hz' % (h_freq, l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1034: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' from %0.2g - %0.2g Hz' % (h_freq, l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1772: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('- Lower passband edge: %0.2f' % (l_freq,))\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1773: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' (%s cutoff frequency: %0.2f Hz)' % (\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1799: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('- Upper passband edge: %0.2f Hz' % (h_freq,))\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1800: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' (%s cutoff frequency: %0.2f Hz)' % (\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1822: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  float(min(h_check, l_check)),)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICA completed\n",
      "5. Applying baseline correction...\n",
      "Applying baseline correction (mode: mean)\n",
      "Baseline correction completed\n",
      "[[0.8810837 ]\n",
      " [0.87768567]\n",
      " [0.82781464]\n",
      " [0.92183346]\n",
      " [0.883632  ]\n",
      " [0.8369908 ]\n",
      " [0.8843969 ]\n",
      " [0.88429546]\n",
      " [0.81707436]\n",
      " [0.8676273 ]\n",
      " [0.9148272 ]\n",
      " [0.922066  ]\n",
      " [0.8101219 ]\n",
      " [0.7273692 ]\n",
      " [0.83795357]\n",
      " [0.8766037 ]\n",
      " [0.6507389 ]\n",
      " [0.7897131 ]\n",
      " [0.91054595]\n",
      " [0.9141953 ]\n",
      " [0.8906377 ]\n",
      " [0.9163796 ]\n",
      " [0.7924859 ]\n",
      " [0.90501493]\n",
      " [0.88552177]\n",
      " [0.85247445]\n",
      " [0.7833756 ]\n",
      " [0.7813268 ]\n",
      " [0.9116129 ]\n",
      " [0.9105847 ]\n",
      " [0.9056789 ]\n",
      " [0.85142154]\n",
      " [0.9209563 ]\n",
      " [0.9153217 ]\n",
      " [0.89807105]\n",
      " [0.87573   ]\n",
      " [0.8899173 ]\n",
      " [0.88219935]\n",
      " [0.89894015]\n",
      " [0.8908807 ]\n",
      " [0.82569623]\n",
      " [0.9207742 ]\n",
      " [0.9201482 ]\n",
      " [0.89967376]\n",
      " [0.841427  ]\n",
      " [0.8329116 ]\n",
      " [0.90828705]\n",
      " [0.89366376]\n",
      " [0.9021751 ]\n",
      " [0.8609036 ]\n",
      " [0.8522613 ]\n",
      " [0.8883957 ]\n",
      " [0.76264393]\n",
      " [0.8311075 ]\n",
      " [0.90432096]\n",
      " [0.8921838 ]\n",
      " [0.9181649 ]\n",
      " [0.9403085 ]\n",
      " [0.6598822 ]\n",
      " [0.9329643 ]\n",
      " [0.92724264]\n",
      " [0.9146163 ]\n",
      " [0.9036095 ]\n",
      " [0.9053199 ]\n",
      " [0.91639197]\n",
      " [0.9007675 ]\n",
      " [0.896363  ]\n",
      " [0.8840005 ]\n",
      " [0.86250615]\n",
      " [0.88852626]\n",
      " [0.8893603 ]\n",
      " [0.8504533 ]\n",
      " [0.8308231 ]\n",
      " [0.8995945 ]\n",
      " [0.9072867 ]]\n",
      "[{'class': '1', 'probability': 0.8810837268829346}]\n",
      "Extracting EDF parameters from /Users/hansandreanto/Development/capstone-project/edf_dataset_2/MDD_S6_EC.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 77311  =      0.000 ...   301.996 secs...\n",
      "1. Applying bandpass filter...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 60 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 60.00 Hz\n",
      "- Upper transition bandwidth: 15.00 Hz (-6 dB cutoff frequency: 67.50 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n",
      "Setting up band-stop filter from 48 - 52 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 48.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 48.25 Hz)\n",
      "- Upper passband edge: 51.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 51.75 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n",
      "Bandpass filtering completed\n",
      "2. Removing bad channels...\n",
      "Initial channels: ['EEG Fp1-LE', 'EEG F3-LE', 'EEG C3-LE', 'EEG P3-LE', 'EEG O1-LE', 'EEG F7-LE', 'EEG T3-LE', 'EEG T5-LE', 'EEG Fz-LE', 'EEG Fp2-LE', 'EEG F4-LE', 'EEG C4-LE', 'EEG P4-LE', 'EEG O2-LE', 'EEG F8-LE', 'EEG T4-LE', 'EEG T6-LE', 'EEG Cz-LE', 'EEG Pz-LE', 'EEG A2-A1']\n",
      "Dropping channels: ['EEG A2-A1']\n",
      "Final channels: ['Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'T5', 'Fz', 'Fp2', 'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz']\n",
      "Dropping channels to keep only the expected 17 channels: ['T5', 'Fz']\n",
      "Bad channels removed\n",
      "3. Creating epochs...\n",
      "Not setting metadata\n",
      "75 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 75 events and 1280 original time points ...\n",
      "0 bad epochs dropped\n",
      "Epoching completed. Final data shape: (75, 17, 1280, 1)\n",
      "4. Applying ICA...\n",
      "Preprocessing ICA for 75 epochs...\n",
      "Fitting ICA to data using 2 channels (please be patient, this may take a while)\n",
      "Selecting by number: 2 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1008: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('Setting up band-pass filter from %0.2g - %0.2g Hz'\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1008: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('Setting up band-pass filter from %0.2g - %0.2g Hz'\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1724: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  l_freq = cast(l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1729: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  h_freq = cast(h_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1034: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' from %0.2g - %0.2g Hz' % (h_freq, l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1034: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' from %0.2g - %0.2g Hz' % (h_freq, l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1772: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('- Lower passband edge: %0.2f' % (l_freq,))\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1773: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' (%s cutoff frequency: %0.2f Hz)' % (\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1799: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('- Upper passband edge: %0.2f Hz' % (h_freq,))\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1800: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' (%s cutoff frequency: %0.2f Hz)' % (\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1822: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  float(min(h_check, l_check)),)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA took 0.1s.\n",
      "Applying ICA to Epochs instance\n",
      "    Transforming to ICA space (2 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 2 PCA components\n",
      "ICA completed\n",
      "5. Applying baseline correction...\n",
      "Applying baseline correction (mode: mean)\n",
      "Baseline correction completed\n",
      "[('P4_964 <= -0.00', -0.010750992255603732), ('P3_21 > 0.00', -0.010234267848386814), ('-0.00 < T6_507 <= 0.00', -0.009097230785845444), ('0.00 < C4_988 <= 0.00', -0.009006919096134611), ('Pz_985 > 0.00', 0.008721911964479094), ('O2_572 > 0.00', -0.008524995418641722), ('Fp2_990 > 0.00', -0.008196115947299043), ('Cz_761 <= -0.00', -0.008064980799966497), ('0.00 < C4_167 <= 0.00', 0.007947887588628652), ('-0.00 < F4_233 <= 0.00', -0.004187127098497771)]\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from mne.preprocessing import ICA\n",
    "from lime import lime_tabular\n",
    "\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Function to rename channels and drop specified channels based on conditions\n",
    "def process_channels(raw_data):\n",
    "    \"\"\"\n",
    "    Process and standardize EEG channels to keep only the 17 most common channels.\n",
    "    \"\"\"\n",
    "    print(f\"Initial channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Initialize a list to hold channels to drop\n",
    "    channels_to_drop = []\n",
    "\n",
    "    # Create mapping for channel renaming\n",
    "    rename_map = {}\n",
    "    for name in raw_data.ch_names:\n",
    "        if any(x in name for x in ['23A-23R', '24A-24R', 'A2-A1']):\n",
    "            channels_to_drop.append(name)\n",
    "        else:\n",
    "            new_name = name.replace('EEG ', '').replace('-LE', '')\n",
    "            rename_map[name] = new_name\n",
    "\n",
    "    # Drop unwanted channels\n",
    "    if channels_to_drop:\n",
    "        print(f\"Dropping channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Rename remaining channels\n",
    "    raw_data.rename_channels(rename_map)\n",
    "\n",
    "    print(f\"Final channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Define the 17 most common channels\n",
    "    expected_channels = [\n",
    "        'Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'Fp2',\n",
    "        'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz'\n",
    "    ]\n",
    "\n",
    "    # Keep only the expected channels\n",
    "    channels_to_keep = set(expected_channels)\n",
    "    channels_to_drop = [\n",
    "        ch for ch in raw_data.ch_names if ch not in channels_to_keep]\n",
    "\n",
    "    if channels_to_drop:\n",
    "        print(\n",
    "            f\"Dropping channels to keep only the expected 17 channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Verify we have the expected number of channels (should be 17)\n",
    "    if len(raw_data.ch_names) != len(expected_channels):\n",
    "        print(\n",
    "            f\"Warning: Expected {len(expected_channels)} channels, got {len(raw_data.ch_names)}\")\n",
    "        print(f\"Missing: {set(expected_channels) - set(raw_data.ch_names)}\")\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def read_eeg_file(file_path):\n",
    "    # Read the raw data\n",
    "    raw_data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def bandpass_filter(data, l_freq, h_freq, notch_freq=None):\n",
    "    filtered_data = data.copy()\n",
    "\n",
    "    # Apply bandpass first\n",
    "    filtered_data.filter(l_freq=l_freq, h_freq=h_freq,\n",
    "                         method='fir', phase='zero')\n",
    "\n",
    "    # If using notch, apply with wider bandwidth\n",
    "    if notch_freq is not None:\n",
    "        filtered_data.notch_filter(freqs=notch_freq, notch_widths=2.0)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "ica_channels = ['Fp1', 'Fp2']\n",
    "\n",
    "\n",
    "def preprocess_ICA(epochs, n_components):\n",
    "    \"\"\"\n",
    "    Apply Independent Component Analysis (ICA) to the epochs data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : mne.Epochs\n",
    "        The epochs data to process.\n",
    "    n_components : int\n",
    "        The number of components to extract.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ica : ICA\n",
    "        The fitted ICA object.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Preprocessing ICA for {len(epochs)} epochs...\")  # Print the number of epochs being processed\n",
    "\n",
    "    ica = ICA(n_components=n_components, random_state=97, max_iter=800)\n",
    "    # Use the epochs directly\n",
    "    ica.fit(epochs.copy().pick_channels(ica_channels))\n",
    "    return ica\n",
    "\n",
    "\n",
    "def create_epochs(processed_data, duration=5.0, overlap=1.0):\n",
    "    \"\"\"\n",
    "    Create epochs from continuous EEG data and format for CNN input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_data : mne.io.Raw\n",
    "        The raw EEG data\n",
    "    duration : float\n",
    "        Duration of each epoch in seconds\n",
    "    overlap : float\n",
    "        Overlap between epochs in seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_array : numpy.ndarray\n",
    "        The epoched data formatted for CNN (samples, channels, timepoints, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create epochs\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        processed_data,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    # Drop bad epochs\n",
    "    epochs.drop_bad()\n",
    "\n",
    "    # Get data and reshape for CNN\n",
    "    # Shape will be (n_epochs, n_channels, n_timepoints)\n",
    "    data = epochs.get_data()\n",
    "\n",
    "    # Add channel dimension for CNN: (n_epochs, n_channels, n_timepoints, 1)\n",
    "    data = data[..., np.newaxis]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_eeg(raw_data, l_freq=0.5, h_freq=60.0, notch_freq=50.0, n_components=2, epoch_duration=5.0, epoch_overlap=1.0):\n",
    "    \"\"\"\n",
    "    Complete EEG preprocessing pipeline: filter -> bad channel removal -> epoching -> ICA -> baseline correction\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(f\"\\nProcessing file: {filename}\")\n",
    "\n",
    "        # Make a copy of the raw data to prevent modifications to original\n",
    "        processed_raw = raw_data.copy()\n",
    "\n",
    "        # 1. Bandpass filtering\n",
    "        print(\"1. Applying bandpass filter...\")\n",
    "        try:\n",
    "            bandpass_filter(processed_raw, l_freq, h_freq, notch_freq)\n",
    "            print(\"Bandpass filtering completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during bandpass filtering: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 2. Bad channel removal\n",
    "        print(\"2. Removing bad channels...\")\n",
    "        try:\n",
    "            processed_raw = process_channels(raw_data=processed_raw)\n",
    "            print(\"Bad channels removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during bad channel removal: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 3. Epoching\n",
    "        print(\"3. Creating epochs...\")\n",
    "        try:\n",
    "            epochs = mne.make_fixed_length_epochs(\n",
    "                processed_raw,\n",
    "                duration=epoch_duration,\n",
    "                overlap=epoch_overlap,\n",
    "                preload=True\n",
    "            )\n",
    "\n",
    "            # Drop bad epochs\n",
    "            epochs.drop_bad()\n",
    "\n",
    "            # Get data and reshape for CNN\n",
    "            data = epochs.get_data()\n",
    "            data = data[..., np.newaxis]  # Add channel dimension for CNN\n",
    "\n",
    "            print(f\"Epoching completed. Final data shape: {data.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoching: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 4. ICA\n",
    "        print(\"4. Applying ICA...\")\n",
    "        try:\n",
    "            # Pass the epochs object\n",
    "            ica = preprocess_ICA(epochs, n_components)\n",
    "            ica.apply(epochs)  # Apply ICA to the epochs\n",
    "            print(\"ICA completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ICA: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 5. Baseline correction\n",
    "        print(\"5. Applying baseline correction...\")\n",
    "        try:\n",
    "            # Apply baseline correction over the entire epoch\n",
    "            epochs.apply_baseline((None, None))\n",
    "            print(\"Baseline correction completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during baseline correction: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        return data  # Return the processed data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"General preprocessing error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_eeg_data(file_path, model):\n",
    "    try:\n",
    "        # Read the EEG file\n",
    "        raw_data = read_eeg_file(file_path)\n",
    "\n",
    "        # Preprocess the EEG data using your existing preprocessing function\n",
    "        processed_data = preprocess_eeg(\n",
    "            raw_data,\n",
    "            l_freq=0.5,\n",
    "            h_freq=60.0,\n",
    "            notch_freq=50.0,\n",
    "            n_components=2,\n",
    "            epoch_duration=5.0,\n",
    "            epoch_overlap=1.0\n",
    "        )\n",
    "\n",
    "        # Ensure processed_data is in the correct shape\n",
    "        processed_data = np.squeeze(processed_data)\n",
    "\n",
    "        # Reshape data to match model's expected input\n",
    "        reshaped_data = processed_data.reshape(\n",
    "            processed_data.shape[0], processed_data.shape[1], -1)\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        input_tensor = tf.convert_to_tensor(reshaped_data, dtype=tf.float32)\n",
    "\n",
    "        # Ensure the input tensor is 3D: (n_epochs, n_channels, n_timepoints)\n",
    "        if len(input_tensor.shape) == 2:  # If it's 2D, add a channel dimension\n",
    "            input_tensor = tf.expand_dims(input_tensor, axis=-1)\n",
    "\n",
    "        # Make prediction\n",
    "        predict_fn = model.signatures['serving_default']\n",
    "        predictions = predict_fn(input_tensor)\n",
    "        output_key = list(predictions.keys())[0]\n",
    "        preds = predictions[output_key].numpy()\n",
    "\n",
    "        # Apply a threshold to determine class predictions\n",
    "        threshold = 0.5\n",
    "        class_predictions = (preds[0] > threshold).astype(int)\n",
    "\n",
    "        print(preds)\n",
    "\n",
    "        return [{\n",
    "            \"class\": str(class_prediction),\n",
    "            \"probability\": float(prob)\n",
    "        } for class_prediction, prob in zip(class_predictions, preds[0])]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing EEG data: {str(e)}\")\n",
    "        return [{\"error\": f\"Failed to process EEG data: {str(e)}\"}]\n",
    "\n",
    "    \n",
    "\n",
    "def load_model(model_path):\n",
    "    try:\n",
    "        # Print path to debug\n",
    "        print(f\"Looking for model at: {model_path}\")\n",
    "        print(f\"Directory exists: {os.path.exists(model_path)}\")\n",
    "\n",
    "        # Load model\n",
    "        model = tf.saved_model.load(model_path)\n",
    "        print(\"Model loaded successfully\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    # Specify the path to your EDF file and model\n",
    "edf_file_path = './edf_dataset_2/MDD_S6_EC.edf'\n",
    "model_path = './saved_model/1d_cnn/1st_model/'\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Example usage\n",
    "if model is not None:\n",
    "    results = process_eeg_data(edf_file_path, model)\n",
    "    print(results)\n",
    "    \n",
    "else:\n",
    "    print(\"Model could not be loaded. Predictions cannot be made.\")\n",
    "\n",
    "# Define your actual channel names (replace with your specific channel labels)\n",
    "channel_names = [\n",
    "    'Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'Fp2',\n",
    "    'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz'\n",
    "]\n",
    "\n",
    "\n",
    "def explain_prediction(file_path, model):\n",
    "    # First get the processed data using existing function\n",
    "    raw_data = read_eeg_file(file_path)\n",
    "    processed_data = preprocess_eeg(\n",
    "        raw_data,\n",
    "        l_freq=0.5,\n",
    "        h_freq=60.0,\n",
    "        notch_freq=50.0,\n",
    "        n_components=2,\n",
    "        epoch_duration=5.0,\n",
    "        epoch_overlap=1.0\n",
    "    )\n",
    "\n",
    "    # Reshape the processed data to 2D format for LIME\n",
    "    flattened_data = processed_data.reshape(processed_data.shape[0], -1)\n",
    "\n",
    "    # Create wrapper function for model predictions\n",
    "    def model_predict(data):\n",
    "        # Reshape back to model's expected format\n",
    "        reshaped_data = data.reshape(-1, 17, 1280)\n",
    "        data_tensor = tf.convert_to_tensor(reshaped_data, dtype=tf.float32)\n",
    "        predict_fn = model.signatures['serving_default']\n",
    "        predictions = predict_fn(data_tensor)\n",
    "        output_key = list(predictions.keys())[0]\n",
    "        probs = predictions[output_key].numpy()\n",
    "\n",
    "        # Convert single probability to two-class probability distribution\n",
    "        two_class_probs = np.zeros((probs.shape[0], 2))\n",
    "        two_class_probs[:, 1] = probs.flatten()\n",
    "        two_class_probs[:, 0] = 1 - probs.flatten()\n",
    "        return two_class_probs\n",
    "\n",
    "    # Create LIME explainer\n",
    "    explainer = LimeTabularExplainer(\n",
    "        training_data=flattened_data,\n",
    "        feature_names=[f\"{channel}_{t}\" for channel in channel_names\n",
    "                       for t in range(processed_data.shape[2])],\n",
    "        class_names=['Normal', 'MDD'],\n",
    "        mode='classification'\n",
    "    )\n",
    "\n",
    "    # Get explanation for first instance\n",
    "    instance_to_explain = flattened_data[0]\n",
    "    explanation = explainer.explain_instance(\n",
    "        instance_to_explain,\n",
    "        model_predict,\n",
    "        num_features=10\n",
    "    )\n",
    "\n",
    "    return explanation\n",
    "\n",
    "# Usage\n",
    "explanation = explain_prediction(edf_file_path, model)\n",
    "# View results\n",
    "print(explanation.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure processed_data is in the correct shape\n",
    "# processed_data = np.squeeze(processed_data)\n",
    "\n",
    "# # Reshape data to match model's expected input\n",
    "# reshaped_data = processed_data.reshape(\n",
    "#     processed_data.shape[0], processed_data.shape[1], -1)\n",
    "\n",
    "# # Convert to tensor and add batch dimension\n",
    "# input_tensor = tf.convert_to_tensor(reshaped_data, dtype=tf.float32)\n",
    "\n",
    "# # Ensure the input tensor is 3D: (n_epochs, n_channels, n_timepoints)\n",
    "# if len(input_tensor.shape) == 2:  # If it's 2D, add a channel dimension\n",
    "#     input_tensor = tf.expand_dims(input_tensor, axis=-1)\n",
    "\n",
    "# # Make prediction\n",
    "# predictions = predict_fn(input_tensor)\n",
    "# output_key = list(predictions.keys())[0]\n",
    "# preds = predictions[output_key].numpy()\n",
    "\n",
    "# # Debugging: Print the raw predictions output\n",
    "# print(f\"Raw predictions output: {preds}\")\n",
    "\n",
    "# # Apply a threshold to determine class predictions\n",
    "# threshold = 0.55\n",
    "# class_predictions = (preds[0] > threshold).astype(int)\n",
    "\n",
    "# return [{\n",
    "#     \"class\": str(class_prediction),\n",
    "#     \"probability\": float(prob)\n",
    "# } for class_prediction, prob in zip(class_predictions, preds[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Assuming you have the following variables from the previous code\n",
    "# X_data, y_labels, class_counts\n",
    "\n",
    "# Concatenate the X_data list into a single numpy array\n",
    "X = np.concatenate(X_data, axis=0)\n",
    "\n",
    "# Convert y_labels to a numpy array\n",
    "y = np.array(y_labels)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=X.shape[1:]))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "y_pred = (model.predict(X) > 0.5).astype(int)\n",
    "acc = accuracy_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the following variables from the previous code\n",
    "# X_data, y_labels, class_counts\n",
    "\n",
    "# Concatenate the X_data list into a single numpy array\n",
    "X = np.concatenate(X_data, axis=0)\n",
    "\n",
    "# Convert y_labels to a numpy array\n",
    "y = np.array(y_labels)\n",
    "\n",
    "# Check the input shape\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=X.shape[1:]))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "# Adjust the pooling size to match the input shape\n",
    "model.add(MaxPooling2D((1, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X) > 0.5).astype(int)\n",
    "acc = accuracy_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_eeg(raw_data, l_freq=0.5, h_freq=50.0, n_components=5, epoch_duration=5.0, epoch_overlap=1.0):\n",
    "#     \"\"\"\n",
    "#     Complete EEG preprocessing pipeline: bandpass -> ICA -> epoching\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     raw_data : mne.io.Raw\n",
    "#         Raw EEG data\n",
    "#     l_freq : float\n",
    "#         Lower frequency bound for bandpass filter\n",
    "#     h_freq : float\n",
    "#         Higher frequency bound for bandpass filter\n",
    "#     n_components : int\n",
    "#         Number of ICA components\n",
    "#     epoch_duration : float\n",
    "#         Duration of each epoch in seconds\n",
    "#     epoch_overlap : float\n",
    "#         Overlap between epochs in seconds\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     epochs_array : numpy.ndarray\n",
    "#         Preprocessed and epoched data ready for CNN (samples, channels, timepoints, 1)\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         print(f\"Processing file: {raw_data.filenames}\")\n",
    "\n",
    "#         processed_raw = process_channels(raw_data=raw_data)\n",
    "\n",
    "#         # 1. Bandpass filtering\n",
    "#         print(\"Applying bandpass filter...\")\n",
    "#         bandpass_filter(processed_raw, 0.5, 45)\n",
    "\n",
    "#         # 2. ICA\n",
    "#         print(\"Applying ICA...\")\n",
    "#         preprocess_ICA(processed_raw, 5)\n",
    "\n",
    "#         # Find and remove EOG artifacts\n",
    "#         # eog_indices, eog_scores = ica.find_bads_eog(raw_data)\n",
    "#         # if eog_indices:\n",
    "#         #     print(f'Found {len(eog_indices)} EOG components')\n",
    "#         #     ica.exclude = eog_indices\n",
    "#         # ica.apply(raw_data)\n",
    "\n",
    "#         # 3. Epoching\n",
    "#         print(\"Creating epochs...\")\n",
    "#         create_epochs(processed_raw)\n",
    "\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during preprocessing: {str(e)}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import Required Libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model  \u001b[38;5;66;03m# Save model in SavedModel format\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n",
      "File \u001b[0;32m~/Development/capstone-project/capstone_env/lib/python3.11/site-packages/tensorflow/__init__.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autodiff\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n",
      "File \u001b[0;32m~/Development/capstone-project/capstone_env/lib/python3.11/site-packages/tensorflow/_api/v2/autograph/__init__.py:23\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Conversion of eager-style Python into TensorFlow graph code.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mNOTE: In TensorFlow 2.0, AutoGraph is automatically applied when using\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_code\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_graph\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1080\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1631\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model  # Save model in SavedModel format\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "import re  # Import the regular expressions module\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "# To unzip the edf_dataset\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# EDFlib and Data Preprocesing module\n",
    "from mne.preprocessing import ICA, create_eog_epochs\n",
    "import mne\n",
    "from pyedflib import highlevel\n",
    "import pyedflib as plib\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_file(url, save_path):\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"File already exists at '{save_path}'. Skipping download.\")\n",
    "        return  # Exit the function if the file exists\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open the file in binary write mode and save the content\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully and saved to '{save_path}'\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "# Specify the URL and the path where you want to save the file\n",
    "url = 'https://figshare.com/ndownloader/articles/4244171/versions/2'\n",
    "# Change this to your desired path\n",
    "save_path = './edf_dataset.zip'\n",
    "\n",
    "# Call the function to download the file\n",
    "download_file(url, save_path)\n",
    "\n",
    "\n",
    "def unzip_file(zip_file_path, extract_to_folder):\n",
    "\n",
    "    # Check if the directory exist\n",
    "\n",
    "    if os.path.exists(extract_to_folder):\n",
    "        print(f\"Directory '{extract_to_folder} already exists\")\n",
    "        return  # Exit the function if the directory\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extract_to_folder, exist_ok=True)\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extract all the contents into the specified folder\n",
    "        zip_ref.extractall(extract_to_folder)\n",
    "\n",
    "\n",
    "# Specify the path to the zip file and the extraction folder\n",
    "zip_file_path = './edf_dataset.zip'\n",
    "# Change this if needed\n",
    "extract_to_folder = './edf_dataset_2'\n",
    "\n",
    "# Call the function to unzip\n",
    "unzip_file(zip_file_path, extract_to_folder)\n",
    "\n",
    "\n",
    "# edf_directory\n",
    "edf_directory = \"./edf_dataset_2\"\n",
    "# Loop through all files in the specified directory\n",
    "for filename in os.listdir(edf_directory):\n",
    "    # Check if the filename contains spaces\n",
    "    if ' ' in filename:\n",
    "        # Create a new filename by replacing multiple spaces with a single underscore\n",
    "        # Replace one or more spaces with a single underscore\n",
    "        new_filename = re.sub(r'\\s+', '_', filename)\n",
    "\n",
    "        # Get the full path for the old and new filenames\n",
    "        old_file = os.path.join(edf_directory, filename)\n",
    "        new_file = os.path.join(edf_directory, new_filename)\n",
    "\n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "        print(f'Renamed: \"{filename}\" to \"{new_filename}\"')\n",
    "    elif '6931959' in filename:\n",
    "        new_filename = filename.replace('6921959_', '')\n",
    "\n",
    "        # Get the full path for the old and new filenames\n",
    "        old_file = os.path.join(edf_directory, filename)\n",
    "        new_file = os.path.join(edf_directory, new_filename)\n",
    "\n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "        print(f'Renamed: \"{filename}\" to \"{new_filename}\"')\n",
    "    elif '6921143' in filename:\n",
    "        new_filename = filename.replace('6921143_', '')\n",
    "        # Get the full path for the old and new filenames\n",
    "        old_file = os.path.join(edf_directory, filename)\n",
    "        new_file = os.path.join(edf_directory, new_filename)\n",
    "\n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "        print(f'Renamed: \"{filename}\" to \"{new_filename}\"')\n",
    "print(\"Renaming complete.\")\n",
    "\n",
    "# Function to rename channels and drop specified channels based on conditions\n",
    "\n",
    "\n",
    "def process_channels(raw_data):\n",
    "    \"\"\"\n",
    "    Process and standardize EEG channels to keep only the 17 most common channels.\n",
    "    \"\"\"\n",
    "    print(f\"Initial channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Initialize a list to hold channels to drop\n",
    "    channels_to_drop = []\n",
    "\n",
    "    # Create mapping for channel renaming\n",
    "    rename_map = {}\n",
    "    for name in raw_data.ch_names:\n",
    "        if any(x in name for x in ['23A-23R', '24A-24R', 'A2-A1']):\n",
    "            channels_to_drop.append(name)\n",
    "        else:\n",
    "            new_name = name.replace('EEG ', '').replace('-LE', '')\n",
    "            rename_map[name] = new_name\n",
    "\n",
    "    # Drop unwanted channels\n",
    "    if channels_to_drop:\n",
    "        print(f\"Dropping channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Rename remaining channels\n",
    "    raw_data.rename_channels(rename_map)\n",
    "\n",
    "    print(f\"Final channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Define the 17 most common channels\n",
    "    expected_channels = [\n",
    "        'Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'Fp2',\n",
    "        'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz'\n",
    "    ]\n",
    "\n",
    "    # Keep only the expected channels\n",
    "    channels_to_keep = set(expected_channels)\n",
    "    channels_to_drop = [\n",
    "        ch for ch in raw_data.ch_names if ch not in channels_to_keep]\n",
    "\n",
    "    if channels_to_drop:\n",
    "        print(\n",
    "            f\"Dropping channels to keep only the expected 17 channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Verify we have the expected number of channels (should be 17)\n",
    "    if len(raw_data.ch_names) != len(expected_channels):\n",
    "        print(\n",
    "            f\"Warning: Expected {len(expected_channels)} channels, got {len(raw_data.ch_names)}\")\n",
    "        print(f\"Missing: {set(expected_channels) - set(raw_data.ch_names)}\")\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "all_edf_files = os.listdir(edf_directory)\n",
    "ec_file_path = [i for i in all_edf_files if i.endswith('EC.edf')]\n",
    "eo_file_path = [i for i in all_edf_files if i.endswith('EO.edf')]\n",
    "task_file_path = [i for i in all_edf_files if i.endswith('TASK.edf')]\n",
    "\n",
    "print(len(all_edf_files), len(ec_file_path),\n",
    "      len(eo_file_path), len(task_file_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Directory containing the EDF files\n",
    "edf_directory = \"./edf_dataset_2\"  # Adjust this path to your dataset location\n",
    "\n",
    "# Initialize lists\n",
    "processed_raw_data = []\n",
    "class_counts = {'Healthy': 0, 'MDD': 0}\n",
    "\n",
    "# Read all EDF files\n",
    "for filename in os.listdir(edf_directory):\n",
    "    if filename.endswith('.edf'):\n",
    "        file_path = os.path.join(edf_directory, filename)\n",
    "        try:\n",
    "            # Read the raw data\n",
    "            raw_data = read_data(file_path)\n",
    "\n",
    "            if raw_data is not None:\n",
    "                processed_raw_data.append(raw_data)\n",
    "                print(f\"Successfully loaded: {filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to load: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nTotal files loaded: {len(processed_raw_data)}\")\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    return data\n",
    "\n",
    "\n",
    "def bandpass_filter(data, l_freq, h_freq):\n",
    "    # Adjust the filter parameters as needed\n",
    "    data.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "\n",
    "\n",
    "def preprocess_ICA(raw, n_components):\n",
    "    print(f\"Preprocessing ICA: {raw.filenames}\")\n",
    "\n",
    "    ica = ICA(n_components=n_components, random_state=97,\n",
    "              max_iter=800)\n",
    "    ica.fit(raw)\n",
    "    return ica\n",
    "\n",
    "\n",
    "def create_epochs(processed_data, duration=5.0, overlap=1.0):\n",
    "    \"\"\"\n",
    "    Create epochs from continuous EEG data and format for CNN input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_data : mne.io.Raw\n",
    "        The raw EEG data\n",
    "    duration : float\n",
    "        Duration of each epoch in seconds\n",
    "    overlap : float\n",
    "        Overlap between epochs in seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_array : numpy.ndarray\n",
    "        The epoched data formatted for CNN (samples, channels, timepoints, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create epochs\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        processed_data,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    # Drop bad epochs\n",
    "    epochs.drop_bad()\n",
    "\n",
    "    # Get data and reshape for CNN\n",
    "    # Shape will be (n_epochs, n_channels, n_timepoints)\n",
    "    data = epochs.get_data()\n",
    "\n",
    "    # Add channel dimension for CNN: (n_epochs, n_channels, n_timepoints, 1)\n",
    "    data = data[..., np.newaxis]\n",
    "\n",
    "    return data\n",
    "def preprocess_eeg(raw_data, l_freq=0.5, h_freq=50.0, n_components=5, epoch_duration=5.0, epoch_overlap=1.0):\n",
    "    \"\"\"\n",
    "    Complete EEG preprocessing pipeline: bandpass -> ICA -> epoching\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nProcessing file: {raw_data.filenames}\")\n",
    "\n",
    "        # Make a copy of the raw data to prevent modifications to original\n",
    "\n",
    "        processed_raw = process_channels(raw_data=raw_data)\n",
    "\n",
    "        # 1. Bandpass filtering\n",
    "        print(\"1. Applying bandpass filter...\")\n",
    "        try:\n",
    "            bandpass_filter(processed_raw, l_freq, h_freq)\n",
    "            print(\"Bandpass filtering completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during bandpass filtering: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 2. ICA\n",
    "        print(\"2. Applying ICA...\")\n",
    "        try:\n",
    "            ica = preprocess_ICA(processed_raw, n_components)\n",
    "            ica.apply(processed_raw)\n",
    "            print(\"ICA completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ICA: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 3. Epoching\n",
    "        print(\"3. Creating epochs...\")\n",
    "        try:\n",
    "            epochs = mne.make_fixed_length_epochs(\n",
    "                processed_raw,\n",
    "                duration=epoch_duration,\n",
    "                overlap=epoch_overlap,\n",
    "                preload=True\n",
    "            )\n",
    "\n",
    "            # Drop bad epochs\n",
    "            epochs.drop_bad()\n",
    "\n",
    "            # Get data and reshape for CNN\n",
    "            data = epochs.get_data()\n",
    "            data = data[..., np.newaxis]  # Add channel dimension for CNN\n",
    "\n",
    "            print(f\"Epoching completed. Final data shape: {data.shape}\")\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoching: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"General preprocessing error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Now process each raw data file\n",
    "X_data = []\n",
    "y_labels = []\n",
    "class_counts = {'Healthy': 0, 'MDD': 0}\n",
    "\n",
    "print(\"\\nStarting preprocessing pipeline...\")\n",
    "for raw_data in processed_raw_data:\n",
    "    filename = os.path.basename(raw_data.filenames[0])\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing: {filename}\")\n",
    "    print(f\"Initial data info:\")\n",
    "    print(f\"Channels: {raw_data.ch_names}\")\n",
    "    print(f\"Sample rate: {raw_data.info['sfreq']} Hz\")\n",
    "    print(f\"Duration: {raw_data.n_times / raw_data.info['sfreq']:.2f} seconds\")\n",
    "\n",
    "    # Apply complete preprocessing pipeline\n",
    "    try:\n",
    "        # First, process the channels\n",
    "        # Make a copy to prevent modifying original\n",
    "        raw_data = process_channels(raw_data.copy())\n",
    "        print(f\"Channels after processing: {raw_data.ch_names}\")\n",
    "\n",
    "        # Apply complete preprocessing pipeline\n",
    "        processed_data = preprocess_eeg(\n",
    "            raw_data,\n",
    "            l_freq=0.5,\n",
    "            h_freq=50.0,\n",
    "            n_components=5,\n",
    "            epoch_duration=5.0,\n",
    "            epoch_overlap=1.0\n",
    "        )\n",
    "\n",
    "        if processed_data is not None:\n",
    "            print(f\"Processed data shape: {processed_data.shape}\")\n",
    "\n",
    "            # Create label (1 for MDD, 0 for Healthy)\n",
    "            label = 1 if filename.startswith('MDD') else 0\n",
    "\n",
    "            # Update counts with number of epochs\n",
    "            if label == 1:\n",
    "                class_counts['MDD'] += processed_data.shape[0]\n",
    "            else:\n",
    "                class_counts['Healthy'] += processed_data.shape[0]\n",
    "\n",
    "            # Append to lists\n",
    "            X_data.append(processed_data)\n",
    "            y_labels.extend([label] * processed_data.shape[0])\n",
    "            print(f\"Successfully processed {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to process {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total files processed: {len(processed_raw_data)}\")\n",
    "print(f\"Total epochs: {len(y_labels)}\")\n",
    "print(f\"Successfully processed files: {len(X_data)}\")\n",
    "print(f\"Failed files: {len(processed_raw_data) - len(X_data)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"MDD epochs: {class_counts['MDD']}\")\n",
    "print(f\"Healthy epochs: {class_counts['Healthy']}\")\n",
    "# print(f\"\\nChannels used: {raw_data.ch_names}\")\n",
    "\n",
    "# # Optional: Print class balance percentage\n",
    "# total_epochs = class_counts['MDD'] + class_counts['Healthy']\n",
    "# print(\"\\nClass balance:\")\n",
    "# print(f\"MDD: {(class_counts['MDD']/total_epochs)*100:.2f}%\")\n",
    "# print(f\"Healthy: {(class_counts['Healthy']/total_epochs)*100:.2f}%\")\n",
    "\n",
    "# Check if we have any processed data\n",
    "if len(X_data) > 0:\n",
    "    try:\n",
    "        # Print shapes before concatenation\n",
    "        print(\"\\nArray shapes before concatenation:\")\n",
    "        for i, arr in enumerate(X_data):\n",
    "            print(f\"Array {i}: shape {arr.shape}\")\n",
    "\n",
    "        # Concatenate data\n",
    "        X = np.concatenate(X_data, axis=0)\n",
    "        y = np.array(y_labels)\n",
    "\n",
    "        # Print final information\n",
    "        print(\"\\nFinal Dataset Information:\")\n",
    "        print(f\"Total samples: {len(X)}\")\n",
    "        print(f\"Healthy samples: {class_counts['Healthy']}\")\n",
    "        print(f\"MDD samples: {class_counts['MDD']}\")\n",
    "        print(f\"Input shape: {X.shape}\")\n",
    "        print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "        # Print channel information\n",
    "        print(f\"\\nChannels used: {raw_data.ch_names}\")\n",
    "\n",
    "        # Print class balance\n",
    "        total_epochs = class_counts['MDD'] + class_counts['Healthy']\n",
    "        print(\"\\nClass balance:\")\n",
    "        print(f\"MDD: {(class_counts['MDD']/total_epochs)*100:.2f}%\")\n",
    "        print(f\"Healthy: {(class_counts['Healthy']/total_epochs)*100:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during final processing: {str(e)}\")\n",
    "        print(\"Checking individual arrays for inconsistencies...\")\n",
    "\n",
    "        # Find arrays with different shapes\n",
    "        base_shape = X_data[0].shape[1:]\n",
    "        for i, arr in enumerate(X_data):\n",
    "            if arr.shape[1:] != base_shape:\n",
    "                print(\n",
    "                    f\"Mismatch at index {i}: expected {base_shape}, got {arr.shape[1:]}\")\n",
    "else:\n",
    "    print(\"\\nNo data was successfully processed!\")\n",
    "\n",
    "\n",
    "# Print original shape\n",
    "print(\"Original shape:\", X.shape)  # Should be (18312, 19, 1280, 1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "# Remove the extra dimension\n",
    "X = X.squeeze(-1)  # Shape becomes (18312, 19, 1280)\n",
    "print(\"Reshaped data shape:\", X.shape)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "y = np.array(y_labels)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    # Input layer - Conv1D\n",
    "    Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=11,\n",
    "        activation='relu',\n",
    "        padding='same',  # Add padding to prevent size issues\n",
    "        input_shape=(17, 1280)\n",
    "    ),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=11,\n",
    "        activation='relu',\n",
    "        padding='same'  # Add padding\n",
    "    ),\n",
    "\n",
    "    # Dropout layer\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # MaxPooling layer\n",
    "    MaxPooling1D(pool_size=4),\n",
    "\n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "\n",
    "    # Dense layer\n",
    "    Dense(100, activation='relu'),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with validation split\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,  # Added validation split\n",
    "    # callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X) > 0.5).astype(int)\n",
    "acc = accuracy_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "MODEL_PATH = './saved_model/1d_cnn/1st_model'\n",
    "tf.keras.models.save_model(model, MODEL_PATH)\n",
    "print(f\"Model saved to: {MODEL_PATH}\")\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "\n",
    "# Load the saved model\n",
    "MODEL_PATH = './saved_model/1d_cnn/1st_model'\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Function to read the EEG data from an EDF file\n",
    "\n",
    "\n",
    "def read_eeg_file(file_path):\n",
    "    # Read the raw data\n",
    "    raw_data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "# Specify the path to your EDF file\n",
    "edf_file_path = './edf_dataset_2/MDD_S1_EC.edf'\n",
    "\n",
    "# Read the EEG file\n",
    "raw_data = read_eeg_file(edf_file_path)\n",
    "\n",
    "# Preprocess the EEG data using your existing preprocessing function\n",
    "# Assuming your preprocessing function is defined as follows:\n",
    "# def preprocess_eeg(raw_data, l_freq=0.5, h_freq=50.0, n_components=5, epoch_duration=5.0, epoch_overlap=1.0):\n",
    "#     # Your existing preprocessing code here\n",
    "processed_data = preprocess_eeg(\n",
    "    raw_data,\n",
    "    l_freq=0.5,\n",
    "    h_freq=50.0,\n",
    "    n_components=5,\n",
    "    epoch_duration=5.0,\n",
    "    epoch_overlap=1.0\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(processed_data)\n",
    "# Convert probabilities to class labels\n",
    "threshold = 0.5\n",
    "predicted_classes = (predictions >= threshold).astype(int)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Model predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def read_eeg_file(file_path):\n",
    "    # Read the raw data\n",
    "    raw_data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def process_eeg_data(file_path):\n",
    "    try:\n",
    "        # Read the EEG file\n",
    "        raw_data = read_eeg_file(file_path)\n",
    "\n",
    "        # Preprocess the EEG data using your existing preprocessing function\n",
    "        processed_data = preprocess_eeg(\n",
    "            raw_data,\n",
    "            l_freq=0.5,\n",
    "            h_freq=50.0,\n",
    "            n_components=5,\n",
    "            epoch_duration=5.0,\n",
    "            epoch_overlap=1.0\n",
    "        )\n",
    "\n",
    "        # Ensure processed_data is in the correct shape\n",
    "        # Assuming processed_data is in the shape (n_epochs, n_channels, n_timepoints, 1)\n",
    "        # Shape becomes (n_epochs, n_channels, n_timepoints)\n",
    "        processed_data = np.squeeze(processed_data)\n",
    "\n",
    "        # Reshape data to match model's expected input\n",
    "        # Flatten the data to (n_epochs, n_timepoints) if necessary\n",
    "        # Shape becomes (n_epochs, n_channels, n_timepoints)\n",
    "        reshaped_data = processed_data.reshape(\n",
    "            processed_data.shape[0], processed_data.shape[1], -1)\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        input_tensor = tf.convert_to_tensor(reshaped_data, dtype=tf.float32)\n",
    "\n",
    "        # Ensure the input tensor is 3D: (n_epochs, n_channels, n_timepoints)\n",
    "        if len(input_tensor.shape) == 2:  # If it's 2D, add a channel dimension\n",
    "            # Shape becomes (n_epochs, n_timepoints, 1)\n",
    "            input_tensor = tf.expand_dims(input_tensor, axis=-1)\n",
    "\n",
    "        # Make prediction\n",
    "        predictions = predict_fn(input_tensor)\n",
    "        output_key = list(predictions.keys())[0]\n",
    "        preds = predictions[output_key].numpy()\n",
    "\n",
    "        return [{\n",
    "            \"class\": str(i),\n",
    "            \"probability\": float(prob)\n",
    "        } for i, prob in enumerate(preds[0])]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing EEG data: {str(e)}\")\n",
    "        return [{\"error\": f\"Failed to process EEG data: {str(e)}\"}]\n",
    "\n",
    "\n",
    "# Specify the path to your EDF file\n",
    "edf_file_path = './edf_dataset_2/H_S9_TASK.edf'\n",
    "\n",
    "# Example usage\n",
    "results = process_eeg_data(edf_file_path)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for model at: ./saved_model/1d_cnn/1st_model/\n",
      "Directory exists: True\n",
      "Model loaded successfully\n",
      "Extracting EDF parameters from /Users/hansandreanto/Development/capstone-project/edf_dataset_2/MDD_S6_EC.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 77311  =      0.000 ...   301.996 secs...\n",
      "1. Applying bandpass filter...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 60 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 60.00 Hz\n",
      "- Upper transition bandwidth: 15.00 Hz (-6 dB cutoff frequency: 67.50 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1008: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('Setting up band-pass filter from %0.2g - %0.2g Hz'\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1008: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('Setting up band-pass filter from %0.2g - %0.2g Hz'\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1724: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  l_freq = cast(l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1729: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  h_freq = cast(h_freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-stop filter from 48 - 52 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 48.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 48.25 Hz)\n",
      "- Upper passband edge: 51.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 51.75 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n",
      "Bandpass filtering completed\n",
      "2. Removing bad channels...\n",
      "Initial channels: ['EEG Fp1-LE', 'EEG F3-LE', 'EEG C3-LE', 'EEG P3-LE', 'EEG O1-LE', 'EEG F7-LE', 'EEG T3-LE', 'EEG T5-LE', 'EEG Fz-LE', 'EEG Fp2-LE', 'EEG F4-LE', 'EEG C4-LE', 'EEG P4-LE', 'EEG O2-LE', 'EEG F8-LE', 'EEG T4-LE', 'EEG T6-LE', 'EEG Cz-LE', 'EEG Pz-LE', 'EEG A2-A1']\n",
      "Dropping channels: ['EEG A2-A1']\n",
      "Final channels: ['Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'T5', 'Fz', 'Fp2', 'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz']\n",
      "Dropping channels to keep only the expected 17 channels: ['T5', 'Fz']\n",
      "Bad channels removed\n",
      "3. Creating epochs...\n",
      "Not setting metadata\n",
      "75 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 75 events and 1280 original time points ...\n",
      "0 bad epochs dropped\n",
      "Epoching completed. Final data shape: (75, 17, 1280, 1)\n",
      "4. Applying ICA...\n",
      "Preprocessing ICA for 75 epochs...\n",
      "Error during ICA: name 'ICA' is not defined\n",
      "Error processing EEG data: tuple index out of range\n",
      "[{'error': 'Failed to process EEG data: tuple index out of range'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1034: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' from %0.2g - %0.2g Hz' % (h_freq, l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1034: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' from %0.2g - %0.2g Hz' % (h_freq, l_freq)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1772: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('- Lower passband edge: %0.2f' % (l_freq,))\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1773: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' (%s cutoff frequency: %0.2f Hz)' % (\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1799: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  logger.info('- Upper passband edge: %0.2f Hz' % (h_freq,))\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1800: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  msg += ' (%s cutoff frequency: %0.2f Hz)' % (\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:1822: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  float(min(h_check, l_check)),)\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n",
      "/Users/hansandreanto/Development/capstone-project/capstone_env/lib/python3.11/site-packages/mne/filter.py:312: DeprecationWarning: Keyword argument 'nyq' is deprecated in favour of 'fs' and will be removed in SciPy 1.12.0.\n",
      "  this_h = firwin(this_N, (prev_freq + this_freq) / 2.,\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Function to rename channels and drop specified channels based on conditions\n",
    "\n",
    "\n",
    "def process_channels(raw_data):\n",
    "    \"\"\"\n",
    "    Process and standardize EEG channels to keep only the 17 most common channels.\n",
    "    \"\"\"\n",
    "    print(f\"Initial channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Initialize a list to hold channels to drop\n",
    "    channels_to_drop = []\n",
    "\n",
    "    # Create mapping for channel renaming\n",
    "    rename_map = {}\n",
    "    for name in raw_data.ch_names:\n",
    "        if any(x in name for x in ['23A-23R', '24A-24R', 'A2-A1']):\n",
    "            channels_to_drop.append(name)\n",
    "        else:\n",
    "            new_name = name.replace('EEG ', '').replace('-LE', '')\n",
    "            rename_map[name] = new_name\n",
    "\n",
    "    # Drop unwanted channels\n",
    "    if channels_to_drop:\n",
    "        print(f\"Dropping channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Rename remaining channels\n",
    "    raw_data.rename_channels(rename_map)\n",
    "\n",
    "    print(f\"Final channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Define the 17 most common channels\n",
    "    expected_channels = [\n",
    "        'Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'Fp2',\n",
    "        'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz'\n",
    "    ]\n",
    "\n",
    "    # Keep only the expected channels\n",
    "    channels_to_keep = set(expected_channels)\n",
    "    channels_to_drop = [\n",
    "        ch for ch in raw_data.ch_names if ch not in channels_to_keep]\n",
    "\n",
    "    if channels_to_drop:\n",
    "        print(\n",
    "            f\"Dropping channels to keep only the expected 17 channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Verify we have the expected number of channels (should be 17)\n",
    "    if len(raw_data.ch_names) != len(expected_channels):\n",
    "        print(\n",
    "            f\"Warning: Expected {len(expected_channels)} channels, got {len(raw_data.ch_names)}\")\n",
    "        print(f\"Missing: {set(expected_channels) - set(raw_data.ch_names)}\")\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    return data\n",
    "\n",
    "\n",
    "def bandpass_filter(data, l_freq, h_freq, notch_freq=None):\n",
    "    filtered_data = data.copy()\n",
    "\n",
    "    # Apply bandpass first\n",
    "    filtered_data.filter(l_freq=l_freq, h_freq=h_freq,\n",
    "                         method='fir', phase='zero')\n",
    "\n",
    "    # If using notch, apply with wider bandwidth\n",
    "    if notch_freq is not None:\n",
    "        filtered_data.notch_filter(freqs=notch_freq, notch_widths=2.0)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "ica_channels = ['Fp1', 'Fp2']\n",
    "\n",
    "\n",
    "def preprocess_ICA(epochs, n_components):\n",
    "    \"\"\"\n",
    "    Apply Independent Component Analysis (ICA) to the epochs data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : mne.Epochs\n",
    "        The epochs data to process.\n",
    "    n_components : int\n",
    "        The number of components to extract.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ica : ICA\n",
    "        The fitted ICA object.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Preprocessing ICA for {len(epochs)} epochs...\")  # Print the number of epochs being processed\n",
    "\n",
    "    ica = ICA(n_components=n_components, random_state=97, max_iter=800)\n",
    "    # Use the epochs directly\n",
    "    ica.fit(epochs.copy().pick_channels(ica_channels))\n",
    "    return ica\n",
    "\n",
    "\n",
    "def create_epochs(processed_data, duration=5.0, overlap=1.0):\n",
    "    \"\"\"\n",
    "    Create epochs from continuous EEG data and format for CNN input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_data : mne.io.Raw\n",
    "        The raw EEG data\n",
    "    duration : float\n",
    "        Duration of each epoch in seconds\n",
    "    overlap : float\n",
    "        Overlap between epochs in seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_array : numpy.ndarray\n",
    "        The epoched data formatted for CNN (samples, channels, timepoints, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create epochs\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        processed_data,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    # Drop bad epochs\n",
    "    epochs.drop_bad()\n",
    "\n",
    "    # Get data and reshape for CNN\n",
    "    # Shape will be (n_epochs, n_channels, n_timepoints)\n",
    "    data = epochs.get_data()\n",
    "\n",
    "    # Add channel dimension for CNN: (n_epochs, n_channels, n_timepoints, 1)\n",
    "    data = data[..., np.newaxis]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_eeg(raw_data, l_freq=0.5, h_freq=60.0, notch_freq=50.0, n_components=2, epoch_duration=5.0, epoch_overlap=1.0):\n",
    "    \"\"\"\n",
    "    Complete EEG preprocessing pipeline: filter -> bad channel removal -> epoching -> ICA -> baseline correction\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(f\"\\nProcessing file: {filename}\")\n",
    "\n",
    "        # Make a copy of the raw data to prevent modifications to original\n",
    "        processed_raw = raw_data.copy()\n",
    "\n",
    "        # 1. Bandpass filtering\n",
    "        print(\"1. Applying bandpass filter...\")\n",
    "        try:\n",
    "            bandpass_filter(processed_raw, l_freq, h_freq, notch_freq)\n",
    "            print(\"Bandpass filtering completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during bandpass filtering: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 2. Bad channel removal\n",
    "        print(\"2. Removing bad channels...\")\n",
    "        try:\n",
    "            processed_raw = process_channels(raw_data=processed_raw)\n",
    "            print(\"Bad channels removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during bad channel removal: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 3. Epoching\n",
    "        print(\"3. Creating epochs...\")\n",
    "        try:\n",
    "            epochs = mne.make_fixed_length_epochs(\n",
    "                processed_raw,\n",
    "                duration=epoch_duration,\n",
    "                overlap=epoch_overlap,\n",
    "                preload=True\n",
    "            )\n",
    "\n",
    "            # Drop bad epochs\n",
    "            epochs.drop_bad()\n",
    "\n",
    "            # Get data and reshape for CNN\n",
    "            data = epochs.get_data()\n",
    "            data = data[..., np.newaxis]  # Add channel dimension for CNN\n",
    "\n",
    "            print(f\"Epoching completed. Final data shape: {data.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoching: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 4. ICA\n",
    "        print(\"4. Applying ICA...\")\n",
    "        try:\n",
    "            # Pass the epochs object\n",
    "            ica = preprocess_ICA(epochs, n_components)\n",
    "            ica.apply(epochs)  # Apply ICA to the epochs\n",
    "            print(\"ICA completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ICA: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 5. Baseline correction\n",
    "        print(\"5. Applying baseline correction...\")\n",
    "        try:\n",
    "            # Apply baseline correction over the entire epoch\n",
    "            epochs.apply_baseline((None, None))\n",
    "            print(\"Baseline correction completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during baseline correction: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        return data  # Return the processed data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"General preprocessing error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_eeg_file(file_path):\n",
    "    # Read the raw data\n",
    "    raw_data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    try:\n",
    "        # Print path to debug\n",
    "        print(f\"Looking for model at: {model_path}\")\n",
    "        print(f\"Directory exists: {os.path.exists(model_path)}\")\n",
    "\n",
    "        # Load model\n",
    "        model = tf.saved_model.load(model_path)\n",
    "        print(\"Model loaded successfully\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_eeg_data(file_path, model):\n",
    "    try:\n",
    "        # Read the EEG file\n",
    "        raw_data = read_eeg_file(file_path)\n",
    "\n",
    "        # Preprocess the EEG data using your existing preprocessing function\n",
    "        processed_data = preprocess_eeg(\n",
    "            raw_data,\n",
    "            l_freq=0.5,\n",
    "            h_freq=60.0,\n",
    "            notch_freq=50.0,\n",
    "            n_components=2,\n",
    "            epoch_duration=5.0,\n",
    "            epoch_overlap=1.0\n",
    "        )\n",
    "\n",
    "        # Ensure processed_data is in the correct shape\n",
    "        processed_data = np.squeeze(processed_data)\n",
    "\n",
    "        # Reshape data to match model's expected input\n",
    "        reshaped_data = processed_data.reshape(\n",
    "            processed_data.shape[0], processed_data.shape[1], -1)\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        input_tensor = tf.convert_to_tensor(reshaped_data, dtype=tf.float32)\n",
    "\n",
    "        # Ensure the input tensor is 3D: (n_epochs, n_channels, n_timepoints)\n",
    "        if len(input_tensor.shape) == 2:  # If it's 2D, add a channel dimension\n",
    "            input_tensor = tf.expand_dims(input_tensor, axis=-1)\n",
    "\n",
    "        # Make prediction\n",
    "        predict_fn = model.signatures['serving_default']\n",
    "        predictions = predict_fn(input_tensor)\n",
    "        output_key = list(predictions.keys())[0]\n",
    "        preds = predictions[output_key].numpy()\n",
    "\n",
    "        # Apply a threshold to determine class predictions\n",
    "        threshold = 0.5\n",
    "        class_predictions = (preds[0] > threshold).astype(int)\n",
    "\n",
    "        print(preds)\n",
    "\n",
    "        return [{\n",
    "            \"class\": str(class_prediction),\n",
    "            \"probability\": float(prob)\n",
    "        } for class_prediction, prob in zip(class_predictions, preds[0])]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing EEG data: {str(e)}\")\n",
    "        return [{\"error\": f\"Failed to process EEG data: {str(e)}\"}]\n",
    "\n",
    "\n",
    "# Specify the path to your EDF file and model\n",
    "edf_file_path = './edf_dataset_2/MDD_S6_EC.edf'\n",
    "model_path = './saved_model/1d_cnn/1st_model/'\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Example usage\n",
    "if model is not None:\n",
    "    results = process_eeg_data(edf_file_path, model)\n",
    "    print(results)\n",
    "else:\n",
    "    print(\"Model could not be loaded. Predictions cannot be made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Function to rename channels and drop specified channels based on conditions\n",
    "\n",
    "\n",
    "def process_channels(raw_data):\n",
    "    \"\"\"\n",
    "    Process and standardize EEG channels to keep only the 17 most common channels.\n",
    "    \"\"\"\n",
    "    print(f\"Initial channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Initialize a list to hold channels to drop\n",
    "    channels_to_drop = []\n",
    "\n",
    "    # Create mapping for channel renaming\n",
    "    rename_map = {}\n",
    "    for name in raw_data.ch_names:\n",
    "        if any(x in name for x in ['23A-23R', '24A-24R', 'A2-A1']):\n",
    "            channels_to_drop.append(name)\n",
    "        else:\n",
    "            new_name = name.replace('EEG ', '').replace('-LE', '')\n",
    "            rename_map[name] = new_name\n",
    "\n",
    "    # Drop unwanted channels\n",
    "    if channels_to_drop:\n",
    "        print(f\"Dropping channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Rename remaining channels\n",
    "    raw_data.rename_channels(rename_map)\n",
    "\n",
    "    print(f\"Final channels: {raw_data.ch_names}\")\n",
    "\n",
    "    # Define the 17 most common channels\n",
    "    expected_channels = [\n",
    "        'Fp1', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'Fp2',\n",
    "        'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'T6', 'Cz', 'Pz'\n",
    "    ]\n",
    "\n",
    "    # Keep only the expected channels\n",
    "    channels_to_keep = set(expected_channels)\n",
    "    channels_to_drop = [\n",
    "        ch for ch in raw_data.ch_names if ch not in channels_to_keep]\n",
    "\n",
    "    if channels_to_drop:\n",
    "        print(\n",
    "            f\"Dropping channels to keep only the expected 17 channels: {channels_to_drop}\")\n",
    "        raw_data.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Verify we have the expected number of channels (should be 17)\n",
    "    if len(raw_data.ch_names) != len(expected_channels):\n",
    "        print(\n",
    "            f\"Warning: Expected {len(expected_channels)} channels, got {len(raw_data.ch_names)}\")\n",
    "        print(f\"Missing: {set(expected_channels) - set(raw_data.ch_names)}\")\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    return data\n",
    "\n",
    "\n",
    "def bandpass_filter(data, l_freq, h_freq, notch_freq=None):\n",
    "    filtered_data = data.copy()\n",
    "\n",
    "    # Apply bandpass first\n",
    "    filtered_data.filter(l_freq=l_freq, h_freq=h_freq,\n",
    "                         method='fir', phase='zero')\n",
    "\n",
    "    # If using notch, apply with wider bandwidth\n",
    "    if notch_freq is not None:\n",
    "        filtered_data.notch_filter(freqs=notch_freq, notch_widths=2.0)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "ica_channels = ['Fp1', 'Fp2']\n",
    "\n",
    "\n",
    "def preprocess_ICA(epochs, n_components):\n",
    "    \"\"\"\n",
    "    Apply Independent Component Analysis (ICA) to the epochs data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : mne.Epochs\n",
    "        The epochs data to process.\n",
    "    n_components : int\n",
    "        The number of components to extract.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ica : ICA\n",
    "        The fitted ICA object.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Preprocessing ICA for {len(epochs)} epochs...\")  # Print the number of epochs being processed\n",
    "\n",
    "    ica = ICA(n_components=n_components, random_state=97, max_iter=800)\n",
    "    # Use the epochs directly\n",
    "    ica.fit(epochs.copy().pick_channels(ica_channels))\n",
    "    return ica\n",
    "\n",
    "\n",
    "def create_epochs(processed_data, duration=5.0, overlap=1.0):\n",
    "    \"\"\"\n",
    "    Create epochs from continuous EEG data and format for CNN input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_data : mne.io.Raw\n",
    "        The raw EEG data\n",
    "    duration : float\n",
    "        Duration of each epoch in seconds\n",
    "    overlap : float\n",
    "        Overlap between epochs in seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_array : numpy.ndarray\n",
    "        The epoched data formatted for CNN (samples, channels, timepoints, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create epochs\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        processed_data,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    # Drop bad epochs\n",
    "    epochs.drop_bad()\n",
    "\n",
    "    # Get data and reshape for CNN\n",
    "    # Shape will be (n_epochs, n_channels, n_timepoints)\n",
    "    data = epochs.get_data()\n",
    "\n",
    "    # Add channel dimension for CNN: (n_epochs, n_channels, n_timepoints, 1)\n",
    "    data = data[..., np.newaxis]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_eeg(raw_data, l_freq=0.5, h_freq=60.0, notch_freq=50.0, n_components=2, epoch_duration=5.0, epoch_overlap=1.0):\n",
    "    \"\"\"\n",
    "    Complete EEG preprocessing pipeline: filter -> bad channel removal -> epoching -> ICA -> baseline correction\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(f\"\\nProcessing file: {filename}\")\n",
    "\n",
    "        # Make a copy of the raw data to prevent modifications to original\n",
    "        processed_raw = raw_data.copy()\n",
    "\n",
    "        # 1. Bandpass filtering\n",
    "        print(\"1. Applying bandpass filter...\")\n",
    "        try:\n",
    "            bandpass_filter(processed_raw, l_freq, h_freq, notch_freq)\n",
    "            print(\"Bandpass filtering completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during bandpass filtering: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 2. Bad channel removal\n",
    "        print(\"2. Removing bad channels...\")\n",
    "        try:\n",
    "            processed_raw = process_channels(raw_data=processed_raw)\n",
    "            print(\"Bad channels removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during bad channel removal: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 3. Epoching\n",
    "        print(\"3. Creating epochs...\")\n",
    "        try:\n",
    "            epochs = mne.make_fixed_length_epochs(\n",
    "                processed_raw,\n",
    "                duration=epoch_duration,\n",
    "                overlap=epoch_overlap,\n",
    "                preload=True\n",
    "            )\n",
    "\n",
    "            # Drop bad epochs\n",
    "            epochs.drop_bad()\n",
    "\n",
    "            # Get data and reshape for CNN\n",
    "            data = epochs.get_data()\n",
    "            data = data[..., np.newaxis]  # Add channel dimension for CNN\n",
    "\n",
    "            print(f\"Epoching completed. Final data shape: {data.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoching: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 4. ICA\n",
    "        print(\"4. Applying ICA...\")\n",
    "        try:\n",
    "            # Pass the epochs object\n",
    "            ica = preprocess_ICA(epochs, n_components)\n",
    "            ica.apply(epochs)  # Apply ICA to the epochs\n",
    "            print(\"ICA completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ICA: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # 5. Baseline correction\n",
    "        print(\"5. Applying baseline correction...\")\n",
    "        try:\n",
    "            # Apply baseline correction over the entire epoch\n",
    "            epochs.apply_baseline((None, None))\n",
    "            print(\"Baseline correction completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during baseline correction: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        return data  # Return the processed data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"General preprocessing error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_eeg_file(file_path):\n",
    "    # Read the raw data\n",
    "    raw_data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    try:\n",
    "        # Print path to debug\n",
    "        print(f\"Looking for model at: {model_path}\")\n",
    "        print(f\"Directory exists: {os.path.exists(model_path)}\")\n",
    "\n",
    "        # Load model\n",
    "        model = tf.saved_model.load(model_path)\n",
    "        print(\"Model loaded successfully\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_eeg_data(file_path, model):\n",
    "    try:\n",
    "        # Read the EEG file\n",
    "        raw_data = read_eeg_file(file_path)\n",
    "\n",
    "        # Preprocess the EEG data using your existing preprocessing function\n",
    "        processed_data = preprocess_eeg(\n",
    "            raw_data,\n",
    "            l_freq=0.5,\n",
    "            h_freq=60.0,\n",
    "            notch_freq=50.0,\n",
    "            n_components=2,\n",
    "            epoch_duration=5.0,\n",
    "            epoch_overlap=1.0\n",
    "        )\n",
    "\n",
    "        # Ensure processed_data is in the correct shape\n",
    "        processed_data = np.squeeze(processed_data)\n",
    "\n",
    "        # Reshape data to match model's expected input\n",
    "        reshaped_data = processed_data.reshape(\n",
    "            processed_data.shape[0], processed_data.shape[1], -1)\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        input_tensor = tf.convert_to_tensor(reshaped_data, dtype=tf.float32)\n",
    "\n",
    "        # Ensure the input tensor is 3D: (n_epochs, n_channels, n_timepoints)\n",
    "        if len(input_tensor.shape) == 2:  # If it's 2D, add a channel dimension\n",
    "            input_tensor = tf.expand_dims(input_tensor, axis=-1)\n",
    "\n",
    "        # Make prediction\n",
    "        predict_fn = model.signatures['serving_default']\n",
    "        predictions = predict_fn(input_tensor)\n",
    "        output_key = list(predictions.keys())[0]\n",
    "        preds = predictions[output_key].numpy()\n",
    "\n",
    "        # Apply a threshold to determine class predictions\n",
    "        threshold = 0.5\n",
    "        class_predictions = (preds[0] > threshold).astype(int)\n",
    "\n",
    "        print(preds)\n",
    "\n",
    "        return [{\n",
    "            \"class\": str(class_prediction),\n",
    "            \"probability\": float(prob)\n",
    "        } for class_prediction, prob in zip(class_predictions, preds[0])]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing EEG data: {str(e)}\")\n",
    "        return [{\"error\": f\"Failed to process EEG data: {str(e)}\"}]\n",
    "\n",
    "\n",
    "# Specify the path to your EDF file and model\n",
    "edf_file_path = './edf_dataset_2/MDD_S6_EC.edf'\n",
    "model_path = './saved_model/1d_cnn/1st_model/'\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Example usage\n",
    "if model is not None:\n",
    "    results = process_eeg_data(edf_file_path, model)\n",
    "    print(results)\n",
    "else:\n",
    "    print(\"Model could not be loaded. Predictions cannot be made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tuning\n",
    "from tensorflow.keras.models import load_model  # Save model in SavedModel format\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    # First Conv Block\n",
    "    Conv1D(\n",
    "        filters=16,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        input_shape=(17, 1280),\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Conv Block\n",
    "    Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Third Conv Block\n",
    "    Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Dense Layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu',\n",
    "          kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "# Compile with lower learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(),\n",
    "             tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.0001\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',  # Use the `.keras` extension\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Data preprocessing\n",
    "X = (X - X.mean()) / (X.std() + 1e-7)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    class_weight={0: 1, 1: 1.2}\n",
    ")\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X) > 0.5).astype(int)\n",
    "acc = accuracy_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Plot training history\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "MODEL_PATH = './saved_model/1d_cnn/2nd_model'\n",
    "tf.saved_model.save(model, MODEL_PATH)\n",
    "print(f\"Model saved to: {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
